The reading notes for paper [Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)

RDDs (Resilient Distributed Datasets) are the core concept for the Spark ecosystem. A RDD is a distrubitued memory distribution.

## What is a RDD?
- Definition
  - source code: [RDD.scala](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala)
- Persistence
- Partitioning


## What are actions and transformations


## Code sample to use RDD

official doc for RDD: https://spark.apache.org/docs/latest/rdd-programming-guide.html
